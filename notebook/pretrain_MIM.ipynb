{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/hikaru-si/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "import os \n",
    "\n",
    "import torch\n",
    "from transformers import LayoutLMv3Tokenizer, AutoConfig\n",
    "\n",
    "sys.path.append('../src')\n",
    "from model import My_DataLoader\n",
    "from model.LayoutLMv3forMIM import LayoutLMv3ForPretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--tokenizer_vocab_dir\", type=str, required=True)\n",
    "parser.add_argument(\"--input_file\", type=str, required=True)\n",
    "parser.add_argument(\"--model_params\", type=str)\n",
    "parser.add_argument(\"--ratio_train\", type=float,default=0.9)\n",
    "parser.add_argument(\"--output_model_dir\", type=str, required=True)\n",
    "parser.add_argument(\"--output_file_name\", type=str, required=True)\n",
    "parser.add_argument(\"--model_name\", type=str, required=True)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "parser.add_argument(\"--leaning_rate\", type=int, default=1e-5)\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1)\n",
    "args_list = [\"--tokenizer_vocab_dir\", \"../data/vocab/tokenizer_vocab/\",\"--input_file\",\n",
    "            \"../data/preprocessing_shared/wpa_10000/\",\n",
    "            \"--output_model_dir\", \"../data/train/model/\", \\\n",
    "            \"--output_file_name\", \"model.param\", \\\n",
    "            \"--batch_size\", \"4\", \\\n",
    "            \"--model_name\", \"microsoft/layoutlmv3-base\", \\\n",
    "              \"--model_params\", \"../data/train/pretrain_lr_1e-4_datasiez_10_batch_32/epoch_3/checkpoint.cpt\"]\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LayoutLMv3Tokenizer(f\"{args.tokenizer_vocab_dir}vocab.json\", f\"{args.tokenizer_vocab_dir}merges.txt\")\n",
    "ids = range(tokenizer.vocab_size)\n",
    "vocab = tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.pkl\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "input_names = os.listdir(args.input_file)\n",
    "for file_name in input_names[0:1]:\n",
    "    print(file_name)\n",
    "    with open(f\"{args.input_file}{file_name}\", \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "        data += d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vlli' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bahcclcsb01/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m vlli\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vlli' is not defined"
     ]
    }
   ],
   "source": [
    "vlli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23274,\n",
       " dict_keys(['input_ids', 'bbox', 'pixel_values', 'label', 'bool_masked_pos', 'alignment_labels']))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0][\"bool_masked_pos\"], data[0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(args\u001b[39m.\u001b[39;49mmodel_params, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/serialization.py:997\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m    995\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 997\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m     \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[1;32m   1001\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# checkpoint = torch.load(args.model_params, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ids = list(range(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint\n"
     ]
    }
   ],
   "source": [
    "if not args.model_params is None:\n",
    "    print(\"load checkpoint\")\n",
    "    checkpoint = torch.load(args.model_params, map_location=torch.device('cpu'))\n",
    "    config = AutoConfig.from_pretrained(args.model_name)\n",
    "    config.num_visual_tokens = 8192\n",
    "    model = LayoutLMv3ForPretraining(config)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    config = AutoConfig.from_pretrained(args.model_name)\n",
    "    config.num_visual_tokens = 8192\n",
    "    model = LayoutLMv3ForPretraining(config)\n",
    "    Roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "    ## embedidng 層の重みをRobertaの重みで初期化\n",
    "    weight_size = model.state_dict()[\"model.embeddings.word_embeddings.weight\"].shape\n",
    "    for i in range(weight_size[0]):\n",
    "      model.state_dict()[\"model.embeddings.word_embeddings.weight\"][i] = \\\n",
    "      Roberta_model.state_dict()[\"embeddings.word_embeddings.weight\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpine13/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y316sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#modelをGPUへ\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpine13/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y316sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel(model, device_ids \u001b[39m=\u001b[39m device_ids)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpine13/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y316sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mdevice_ids[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#modelをGPUへ\n",
    "model = torch.nn.DataParallel(model, device_ids = device_ids)\n",
    "model = model.to(f'cuda:{model.device_ids[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'train_loss_list', 'valid_loss_list', 'model_state_dict', 'optimizer_state_dict'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(checkpoint[\"valid_loss_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  1314,  2628,  3942,  5256,  6570,  7884,  9198, 10512, 11826,\n",
       "        13140, 14454, 15768, 17082, 18396, 19710, 21024, 22338, 23652, 24966,\n",
       "        26280, 27594])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(list(range(len(checkpoint[\"train_loss_list\"]))))*1314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " a =[ 0,  1314,  2628,  3942,  5256,  6570,  7884,  9198, 10512, 11826, 13140, 14454, 15768, 17082, 18396, 19710, 21024, 22338, 23652,24966,26280, 27594]\n",
    " a\n",
    " len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " a =[ 0,  1314,  2628,  3942,  5256,  6570,  7884,  9198, 10512, 11826, 13140, 14454, 15768, 17082, 18396, 19710, 21024, 22338, 23652,24966,26280]\n",
    " len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [0, 1314, 2628, 3942, 5265, 6570, 7884, 9198, 10512, 11826, 13140,13141, 14455, 15769, 17083, 18397, 19711, 21025, 22339, 23653, 24967, 26281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(checkpoint[\"train_loss_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(a , checkpoint[\"train_loss_list\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(checkpoint[\"train_loss_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.max_epochs = 5\n",
    "args.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = range(args.max_epochs)\n",
    "epochs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(2, 5)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = range(checkpoint[\"epoch\"] +1, args.max_epochs)\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(2, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(checkpoint[\"epoch\"] +1, args.max_epochs ):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    " for i in list(range(5)):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  args.model_params is not None:\n",
    "    checkpoint = torch.load(args.model_params, map_location=torch.device('cpu'))\n",
    "    config = AutoConfig.from_pretrained(args.model_name)\n",
    "    config.num_visual_tokens = 8192\n",
    "    model = LayoutLMv3ForPretraining(config)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    config = AutoConfig.from_pretrained(args.model_name)\n",
    "    config.num_visual_tokens = 8192\n",
    "    model = LayoutLMv3ForPretraining(config)\n",
    "    # Roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "    # ## embedidng 層の重みをRobertaの重みで初期化\n",
    "    # weight_size = model.state_dict()[\"model.embeddings.word_embeddings.weight\"].shape\n",
    "    # for i in range(weight_size[0]):\n",
    "    #   model.state_dict()[\"model.embeddings.word_embeddings.weight\"][i] = \\\n",
    "    #   Roberta_model.state_dict()[\"embeddings.word_embeddings.weight\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20946, 2328)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = math.floor(len(data) * args.ratio_train)\n",
    "train_data = data[:n_train]\n",
    "valid_data = data[n_train:]\n",
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "my_dataloader = My_DataLoader.My_Dataloader(vocab, random)\n",
    "train_dataloader = my_dataloader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "valid_dataloader = my_dataloader(valid_data, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ml_loss(text_logits, batch):\n",
    "    t = []\n",
    "    for i in range(len(batch[\"ml_position\"])):\n",
    "        if len(batch[\"ml_position\"][i]) == 0:\n",
    "            continue\n",
    "        t.append(text_logits[i][batch[\"ml_position\"][i]])\n",
    "    if len(t) == 0:\n",
    "        return \n",
    "    predict_word_token = torch.cat(t)\n",
    "    labels = torch.cat(batch[\"ml_label\"])\n",
    "    print(predict_word_token.shape)\n",
    "    print(labels.shape)\n",
    "    # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "    loss = criterion(predict_word_token + 1e-12, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mi_loss(image_logits, batch):\n",
    "    image_logits = image_logits[:,1:]\n",
    "    predict_visual_token = image_logits[batch[\"bool_mi_pos\"]].to(torch.float32)\n",
    "    labels = torch.cat(batch[\"mi_label\"])\n",
    "    # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "    loss = criterion(predict_visual_token+1e-12, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_wpa_loss(wpa_logits, batch):\n",
    "    print(wpa_logits.shape)\n",
    "    w_logits = wpa_logits[:,:512]\n",
    "    #padとlanguage maskのindexを除外\n",
    "    t  = []\n",
    "    for i in range(wpa_logits.shape[0]):\n",
    "        bool_index = torch.ones(512)\n",
    "        bool_index[batch[\"ml_position\"][i]] = 0\n",
    "        bool_index = bool_index * batch[\"attention_mask\"][i]\n",
    "        t.append(bool_index)\n",
    "    bool_indexes = torch.stack(t).to(torch.bool)\n",
    "    predict_label = w_logits[bool_indexes]\n",
    "    labels = batch[\"alignment_labels\"][bool_indexes].to(torch.long)\n",
    "    # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "    loss = criterion(predict_label+1e-12, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bahcclcsb01/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y332sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predict_label\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_label' is not defined"
     ]
    }
   ],
   "source": [
    "predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_word_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bahcclcsb01/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y330sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predict_word_token\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_word_token' is not defined"
     ]
    }
   ],
   "source": [
    "predict_word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ml_loss(text_logits, batch):\n",
    "    t = []\n",
    "    for i in range(len(batch[\"ml_position\"])):\n",
    "        if len(batch[\"ml_position\"][i]) == 0:\n",
    "            continue\n",
    "        t.append(text_logits[i][batch[\"ml_position\"][i]])\n",
    "    if len(t) == 0:\n",
    "        return \n",
    "    predict_word_token = torch.cat(t)\n",
    "    labels = torch.cat(batch[\"ml_label\"])\n",
    "    print(predict_word_token.shape)\n",
    "    print(labels.shape)\n",
    "    # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "    loss = criterion(predict_word_token + 1e-12, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/hikaru-si/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/transformers/modeling_utils.py:713: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([517, 50265])\n",
      "torch.Size([517])\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "# model.train()\n",
    "for epoch in range(args.max_epochs):\n",
    "    for iter, batch in enumerate(train_dataloader):\n",
    "        # inputs = {k: v.to(f'cuda:{model.device_ids[0]}') for k in [\"input_ids, bbox\", \"pixel_values\", \"attention_mask\"]}\n",
    "        inputs = {k: batch[k] for k in [\"input_ids\", \"bbox\", \"pixel_values\", \"attention_mask\", \"bool_mi_pos\"]}\n",
    "        text_logits, image_logits, wpa_logits = model.forward(inputs)\n",
    "    \n",
    "        # # text_logits = logits[:, :512]\n",
    "        # # image_logits = logits[:,513:]\n",
    "        ml_loss = cal_ml_loss(text_logits, batch)\n",
    "        # mi_loss = cal_mi_loss(image_logits, batch)\n",
    "        # wpa_loss = cal_wpa_loss(wpa_logits, batch)\n",
    "\n",
    "        # loss = ml_loss + mi_loss + wpa_loss\n",
    "        break\n",
    "        # # # if loss is None:\n",
    "        # # #     continue\n",
    "        # # # # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # optimizer.zero_grad()\n",
    "        # losses.append(loss.item())\n",
    "        # if iter % math.floor(iter_per_epoch*0.01) == 0:\n",
    "        #     val_loss = validation()\n",
    "        #     print(iter, loss.item())\n",
    "        #     print(iter,\"val\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ml_loss(text_logits, batch):\n",
    "    t = []\n",
    "    for i in range(len(batch[\"ml_position\"])):\n",
    "        if len(batch[\"ml_position\"][i]) == 0:\n",
    "            continue\n",
    "        t.append(text_logits[i][batch[\"ml_position\"][i]])\n",
    "    if len(t) == 0:\n",
    "        return \n",
    "    pre = torch.cat(t)\n",
    "    labels = torch.cat(batch[\"ml_label\"])\n",
    "    return pre, labels\n",
    "    # print(predict_word_token.shape)\n",
    "    # print(labels.shape)\n",
    "    # # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "    # loss = criterion(predict_word_token + 1e-12, labels)\n",
    "    # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre, lab  = ml_loss = cal_ml_loss(text_logits, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42480,   299,   522,   446,   627,   280,   303,   337,   794,  3190,\n",
       "           18,   627,  2740,  3204,   334,  3703,  1974,  9141,  3095,  2557,\n",
       "         1623,    18,   509, 15835,   303,  1199,   402,  1042,   455,   627,\n",
       "          382,  2888, 33390,   299,  9171,  1454,  2577,    13, 38344,   299,\n",
       "         6518,    17,  7855,    13, 16384,   280,   283,   869,   299,  4748,\n",
       "          627,   382,   479,   335,    18,    22,    18,   376,   376,  2447,\n",
       "        31020,    16, 10829,    13,  3460,  1012,   382,  1869, 12574,    16,\n",
       "        40820,    16,  2341,    13,  4051,   465,    17,  1445,    87,   379,\n",
       "          868,  2855,    17,  1839,  2994,  6297,   299,    74,    13,   291,\n",
       "           30,   747,  8046,  7172,  1384,  8590,   489, 38256, 20835,    16,\n",
       "        33431, 20906, 32566,  3080, 33754, 24592,   995,    16,   262, 22210,\n",
       "        17156,    71, 47755,  8590,  9840, 19496,    16, 41481,    21,   291,\n",
       "           30,   261,  3090,  8046, 10948, 11957,  3080, 19585, 31382,    88,\n",
       "           16,  8527,    69,    22, 18770,    24,   271,   337,  2012,  2507,\n",
       "         6465,  2808,  6426,   295,  8412, 19220,    17,  1451,  1670,  1359,\n",
       "           18,  1304,    16,   271,   262,  2808, 15594,    16, 37459,    18,\n",
       "          271,   291,  9199,   339,   280,   295, 37459,  6524,  1503,   479,\n",
       "          764,   766,  1754, 41299,    18,  1808,   351,   283, 23460,    17,\n",
       "         8926,  1122,  7067,   351,   303,  5936,   914, 23460,    17,  2461,\n",
       "         1072,  1223,    30,   339,   767,  1297,    31,   271,  5291,   295,\n",
       "          299,  2020,  3576,   391, 14701,   903,    12,   639,    16,   607,\n",
       "          379,  4258,    16,   263, 29310,  8402,  1460,    16,   380,  3860,\n",
       "         8412, 19220,  1012,    16,  1223,  1667,  2846,  1298,    13,   271,\n",
       "          295,  8822,    17, 14812,  7694, 13909,  1552,  1217,  2017,  1062,\n",
       "           16,   627,   262,  3935,  1365,  1426,   262,   280, 44767,   275,\n",
       "        10155,   299,   825,   379, 19023,  3942,   574,   614,    18,    20,\n",
       "          271,   932,  5545,  2579,   271,  1082,    17,  8700, 13909,    16,\n",
       "          299,   813,   379,  6736,    16, 13604,   335,    13,  1212,  2429,\n",
       "          271,  4040,   436,    13,   271,  1103,   342,    17,   270,   407,\n",
       "          378,   271,   285, 13227,   334,  7802,  9464,   593, 10086,   364,\n",
       "          303,  2768, 15437,   303,  1159,  8256,   816,  1463,   291,   365,\n",
       "          385,    69,  4510,   285,   271, 32827,  1799,  1838,   368,   660,\n",
       "         2813,  3749,   510,    87,   271,  9306,   285,   364,  1913,   339,\n",
       "          271,  6130,  9605, 15279,  7920,  1842,   295,  4785,   285,   295,\n",
       "        14937,  5021,   402,    28,   455,  1359,   285, 12845,   294,   387,\n",
       "          295, 28211,  3596,   285,   271,   600, 32635,  7427,   351,   280,\n",
       "         9952,   510,    87,   994,   402,   813,   455,   368,   271,  7920,\n",
       "        27754,   285,  2087,   295,    19,   278, 18703,  1309,   285,   460,\n",
       "         5976,   283,  1739,  1739, 13211,   609,  1103,   339, 42339, 22929,\n",
       "         6509,   303,   431, 13211,    16,  1048,  5322, 27669,    31,   768,\n",
       "          884,  1873,    17,    24,   295,   464, 39783,    16,   406,  4024,\n",
       "          342,  2639,    18,   406,   431,   860,  3002,   285,   436,   310,\n",
       "          555,   555,   271,   295,   684,   614,    18,    25,   415,   607,\n",
       "           18,    25,    13,   299,    82,   517,    13,  3002,   396,  2121,\n",
       "          402,  4556,   731,    18,    25,   299,  7872,  2937,   339,  2650,\n",
       "         2556,  3002,   809,  9597,    17,  2864,  8320,   280, 39783,    17,\n",
       "         2864,   382,  3835,   271,  6072,   342,   767,    17,  4858, 19310,\n",
       "          262,   345,    17, 42353,   409,  9843,  6794,   364,   396,    74,\n",
       "        30151,    16,   310,  5551,  1367,  2838,  1460,    18,   376,  4451,\n",
       "         1460,   299,  2016,  2937,    13,  1075,   339,   509,  1460,   299,\n",
       "         2016,   382,  1880,   713,  8320,   299,  4678,  3343,  4906,  4756,\n",
       "         1460,    31,  1742,   575,    22,    70,   455])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2108)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pre.argmax(-1) == lab).sum() / len(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_word_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bahcclcsb01/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y331sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predict_word_token\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_word_token' is not defined"
     ]
    }
   ],
   "source": [
    "predict_word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 25,  26,  27,  28,  35,  36,  37,  38,  51,  52,  53,  54,  55,  66,\n",
       "         67,  70,  71,  72,  86,  88,  96, 118, 122, 124, 125, 126, 129, 142,\n",
       "        143, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 160, 161, 172,\n",
       "        183, 192, 200, 229, 235, 236, 237, 238, 250, 251, 266, 297, 298, 299,\n",
       "        309, 310, 311, 312, 313, 314, 315, 316, 334, 335, 336, 354, 355, 356,\n",
       "        357, 358, 359, 360, 435, 436, 437, 465, 466, 467, 468, 469, 494, 495,\n",
       "        496, 497, 498, 499, 500, 501, 502, 503])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"ml_position\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False,  True, False,  True, False, False,  True,  True,\n",
       "        False, False, False,  True, False, False,  True,  True, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False,  True, False,\n",
       "         True,  True,  True,  True,  True, False,  True,  True, False,  True,\n",
       "         True, False, False, False, False,  True, False,  True,  True, False,\n",
       "         True, False,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        False,  True, False,  True, False,  True, False,  True, False, False,\n",
       "         True, False])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_logits[0].argmax(-1)[batch[\"ml_position\"][0]] == batch[\"ml_label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2974,   280,  1767,    16,  1282,   397,  1843,    16,   680,  1095,\n",
       "          364,  4465,    18,  1605,   376,   380,   280,  1063,   706,  1719,\n",
       "         7591,  1367,  1864,   802,  7959,   890,   271,   285,  4465,  4249,\n",
       "          334, 15141,    16,  5410,    17,  8669,  2974,  2151,   295,  1522,\n",
       "         7160, 14210,   262,   555, 11615,  7160,   555, 42654,    13,  5854,\n",
       "         1874,    13,   262,   337,    18,   784,   337,    18,   964,   299,\n",
       "           20,    18,  2025,    16,   469,    18,  2388,   337,    18,  1393,\n",
       "           13,   469,    18,  2030,   337,    18,   358,   299,    20,    18,\n",
       "         1349,    16,   335,    18,   607,    13,  3810,   415,  2025,   469,\n",
       "           18,  1909])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"ml_label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,     4,     4,     4,     4,   272,    30,    29,  5374,   428,\n",
       "         13589,     4,  3493,   848,  6761,     4,     4,     4,     4,     4,\n",
       "         38621,  3565,   287,  7698,  1446, 14340,  1841,   613,   284,    16,\n",
       "         19441,    81,    21,  6351,  6843, 20040, 11348,   466,    16, 19441,\n",
       "            81,    16,  8313,    22,    16,  2011, 14590,   431,   383,     4,\n",
       "             4,     4,     4,  8313,    23,   295, 10304,   263,  1869, 34721,\n",
       "         37079, 11705, 19441,    81,    16,     4,     4, 14735, 14026,    22,\n",
       "           337,  2861,     4,   994,   815, 35259,  3388, 25903, 22123,  8831,\n",
       "             4,     4, 25188,    16,  8249,    31,   335,   522,   626,   378,\n",
       "           285,     4,     4,  2483,  8831,  3023,    16, 25188,    16,  8249]),\n",
       " tensor([  1,   2,   3,   4,  11,  15,  16,  17,  18,  19,  49,  50,  51,  52,\n",
       "          65,  66,  72,  80,  81,  91,  92, 113, 114, 118, 121, 122, 123, 124,\n",
       "         129, 130, 131, 136, 151, 152, 155, 161, 167, 168, 177, 178, 179, 189,\n",
       "         213, 214, 220, 221, 222, 223, 224, 227, 228, 229, 248, 249, 254, 255,\n",
       "         284, 285, 286, 289, 291, 292, 299, 300, 301, 302, 337, 338, 339, 340,\n",
       "         344, 345, 368, 373, 374, 375, 376, 377, 382, 384, 390, 391, 394, 395,\n",
       "         400, 401, 402, 410, 413, 414, 415, 417, 418, 419, 422, 427, 442, 443,\n",
       "         444, 445, 449, 450, 456, 458, 471, 472, 473, 474, 506, 507, 508, 509]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"][2][:100], batch[\"ml_position\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([44427, 34922, 45492, 18593, 11654,  6153,    17,  2708,  4864,   334,\n",
       "         8457,  6918,    94,    16,  8313,    16,   285,  3023,    16, 39276,\n",
       "           16,  8249,   280,  1250,  8249,    16,   262,  4763,   657,  4243,\n",
       "          280,   262,   280,   766,   285, 18593,   271,   814, 11654,    16,\n",
       "          759,   334,   280,  1767,  1778,  5989,   285,   271, 13589,   334,\n",
       "          293,    18,  1801,  1017,   280,   807, 44427, 34922, 45492,  2602,\n",
       "         1585,    31,  3424,    30,  1768,  5172,   759,   285,   271,   932,\n",
       "         3886,  8069,   351,  1253, 38041,   299,    24,   379,   351,  6072,\n",
       "          994,   280,  6725,    16,  8853,  1684,   334,   285,  2556,   516,\n",
       "          368,   299,    26,   446,  3927,   815,  1510,   285,   271,  1788,\n",
       "          673, 29051, 36989, 36507,  2197,   299,    26,   379,  4276,   299,\n",
       "           28,   379])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"ml_label\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5,\n",
       "  15,\n",
       "  16,\n",
       "  20,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  31,\n",
       "  40,\n",
       "  41,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  88,\n",
       "  89,\n",
       "  92,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  113,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  131,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  177,\n",
       "  178,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  198,\n",
       "  200,\n",
       "  201,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  256,\n",
       "  262,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  282,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  306,\n",
       "  307,\n",
       "  312,\n",
       "  316,\n",
       "  326,\n",
       "  327,\n",
       "  330,\n",
       "  331,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  343,\n",
       "  344,\n",
       "  380,\n",
       "  381,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  402,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  428,\n",
       "  429,\n",
       "  434,\n",
       "  438,\n",
       "  439,\n",
       "  444,\n",
       "  467,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  496,\n",
       "  507],\n",
       " tensor([    0,  7730,  1567,    16,   436,     4,   334,   903,  1725,   971,\n",
       "           402,   784,    65,  7104,   400,     4,     4,   285,   685,    16,\n",
       "             4,   295, 11106,     4,     4,     4,     4,     4,   903,  1725,\n",
       "           351,     4,  3045,   342,  1118, 45476,  1365,   397,   627,   944,\n",
       "             4,     4,   408,  6865,   334,   271,  4387,   285,   271,   816,\n",
       "           285,     4,     4,     4, 35359,   860,  2220,   380, 14543,   285,\n",
       "          1783,   295, 11824,  1272,   364,   271,  2826,     4,     4,     4,\n",
       "         23489, 38793,  9893,   295,  5367,     4,     4,     4,  8585,   295,\n",
       "          1381,  6041,  9823,   299,  3284,  2026,    13,  1705,     4,     4,\n",
       "          2362,    18,     4,   478,   351,   476,  6074, 11547,  1821, 17948,\n",
       "           903,    17,  1326,  7730,     4,     4,     4,     4,   759, 46099,\n",
       "           334,   923,  2362,     4,   262,  7181,  1603, 22622,  5937,  1990,\n",
       "             4,     4,     4,     4,   271,  8976,  1385,   342,   979,   342,\n",
       "           673,     4,  1725,  2362,   402,   825,   455,   271, 11482,  4704,\n",
       "           303, 20514,  2073,   903,    17,  1326,  7730,   342,     4,     4,\n",
       "             4,  1725,  3383,   299,   958,   509,    13,   406,     4,     4,\n",
       "             4,   271, 16291,  8976,  1385,   376,  3757,  7743,   339,   280,\n",
       "          1519,   303, 12684,  1350,   285,   903,  1725,     4,     4,  2539,\n",
       "            16,     4,     4,     4,     4,   303,  5744,   339,   286,  5384,\n",
       "            91,  1256,   755,   303, 15137,   339,   271,   923,     4,   285,\n",
       "             4,     4,  3383,   295,  7576,     4,     4,     4,   436,    18,\n",
       "            23,    18,  1511,  2017,   285,  3638,  1778,   334,     4,     4,\n",
       "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "             4,   280,   271,  1029,    18,    87,   601,     4,     4,     4,\n",
       "           436,    18,    23,    18,    21,    18,     4,     4,     4,     4,\n",
       "           280,   271,  1029,    18,    87,    18,     4,  7730, 50010, 29157,\n",
       "         24699,  1331,     4,  1603,  7730,     4,     4,     4,     4,     4,\n",
       "             4,  7730,   408,  6072,   342, 32564,   295,  5367,   400,   271,\n",
       "         11996,   285,     4,   915,   295,  1385,     4,     4,     4,     4,\n",
       "           280,   271,     4,     4,     4, 44742, 18896, 21371,  1650,  1767,\n",
       "          8122,   334, 11824, 10881,   659,   476,     4,     4,   428,   273,\n",
       "           598,    16,     4, 12428,   510,    87,     4,   351,  7579,   280,\n",
       "           271,  8122,  4270, 28362,  1118,  4029,     4,     4,   938,  2519,\n",
       "             4,     4,  7730,   971,     4,     4,     4,     4,     4,  3485,\n",
       "           402,   813,   455,     4,     4,  4373,   295,  3968,    17,    90,\n",
       "           535,  2029,  7730,   408,  1097,   271,  9017,   285,   431, 10897,\n",
       "            18,  3274,  7181,  1603,  7730,    16,   273,   598, 38148, 14079,\n",
       "          1778,  8122, 28362, 10793,  3398,   334,  7181,  1603,  7730,   334,\n",
       "             4,     4, 11688,   971,   402,   746,    65,     4,     4,     4,\n",
       "             4,     4,     4,  9913,   334,  1783,   815,   299, 12683,   387,\n",
       "            13,   295,     4,  2776,   334,  7181,  1603,  7730,    18,  9913,\n",
       "           334,   923,  2776,   408,     4,     4,     4,   271,   273,   598,\n",
       "           860,  7104,   262,  8223,  8122, 17566,  1118,  5810,     4,     4,\n",
       "           364,  7181,  1603,  7730,     4, 10130,  5997,   994,     4,     4,\n",
       "           402,   729,    65,   280,     4,  1167,   364, 14828,  1385,    17,\n",
       "         15750,   339,   673,  2307,   285,  5280,  7730,   295,  1603,    17,\n",
       "          1326,  3673,  3383,    18,   342,   271,  8122,     4,  1713,   271,\n",
       "          6952,   303, 12890,  2081,   337,   923,   788,    16,  7237,   923,\n",
       "          2362,   334,  7181,  1603,     4,     4,     4,  2281,    18,  2034,\n",
       "          2402,   295,  2945,   860,   657, 14383,     4,   271,   273,   598,\n",
       "            16,   884,  7104,   262,  6122,  8122,   280,     4,   402,  1052,\n",
       "            16,     2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"ml_position\"][0].tolist(), batch[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"][0][batch[\"ml_position\"][0].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22662,   271, 11482, 11609,   299,    81,  5384,    91,   446,  7773,\n",
       "          557,  1365,  2336,  7184,   271,   334,   903,  1725,  8976,   400,\n",
       "          271,  5771,   923,  1283,  7934,   271, 35359,    16,   364,  2925,\n",
       "          275,   295,  4268,   903,   979,   342,   903,   380,   814,    18,\n",
       "         3383,    31, 29580,   408,  2621,  4675,  1350,   903,  1725,  5586,\n",
       "         7730,    18,   271,  1350,   285,  7181,  1603,  7730,   295,  3968,\n",
       "           17,    90,   535,  2029,  7730,  2533,   295,  4923,  4373,    17,\n",
       "         1451,  8122, 17948,  7181,   295,  3968,    17,    90,   535,  2029,\n",
       "         7730,   299,  1279,  5802,    13,   273,   598,   285,   657,  7104,\n",
       "          271,  1819,  3638,   364,  2776,   285,  1930,   400,   271,   926,\n",
       "          280,  7181,  1603,  5997,   994,   280,  2399,    16,   557,  3462,\n",
       "         2354, 11824,   476,  1713,    18,   303,  2768,   334, 11688,   971,\n",
       "         6276,   860,  4373,   860,   657,   280,  2399])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"ml_label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 295, 5891, 3330,  335,   18,   22,  436,   18,   26,  335,   18,   28,\n",
       "         1786,  685,  436,   18,   27, 8308,  436,   18,   24, 1788, 3330,  436,\n",
       "           18,   26]),\n",
       " torch.Size([26]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo = torch.zeros(512)\n",
    "for i in batch[\"ml_position\"][2]:\n",
    "  bo[i] = 1\n",
    "bo  = bo.to(torch.bool)\n",
    "batch[\"ml_label\"][2], batch[\"input_ids\"][2][bo].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in batch[\"input_ids\"][2]:\n",
    "  if i == 4:\n",
    "    cnt +=1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  788,   285, 21192,  3943,  6324,  7816,  1603,  4373,  1815,  1815,\n",
       "        12270,    16,   295,  6186])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"ml_label\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  10, 7740, 1577,  ..., 1062,   26,   12],\n",
       "        [  10,  974,   75,  ...,   11,   11,   11],\n",
       "        [  10,   14,   14,  ...,   14,  488,   12],\n",
       "        [  10,   14,   14,  ...,   11,   11,   11]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"] + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"alignment_labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  5,  15,  16,  20,  23,  24,  25,  26,  27,  31,  40,  41,  51,  52,\n",
       "          53,  67,  68,  69,  75,  76,  77,  88,  89,  92, 104, 105, 106, 107,\n",
       "         113, 120, 121, 122, 123, 131, 148, 149, 150, 158, 159, 160, 177, 178,\n",
       "         181, 182, 183, 184, 198, 200, 201, 205, 206, 207, 218, 219, 220, 221,\n",
       "         222, 223, 224, 225, 226, 227, 228, 229, 230, 237, 238, 239, 246, 247,\n",
       "         248, 249, 256, 262, 265, 266, 267, 268, 269, 270, 282, 286, 287, 288,\n",
       "         289, 292, 293, 294, 306, 307, 312, 316, 326, 327, 330, 331, 334, 335,\n",
       "         336, 337, 338, 343, 344, 380, 381, 387, 388, 389, 390, 391, 392, 402,\n",
       "         414, 415, 416, 428, 429, 434, 438, 439, 444, 467, 484, 485, 486, 496,\n",
       "         507]),\n",
       " tensor([13, 14, 15, 20, 21, 22, 27, 28, 34, 49, 52, 53, 54, 55]),\n",
       " tensor([  1,   2,   3,   4,  11,  15,  16,  17,  18,  19,  49,  50,  51,  52,\n",
       "          65,  66,  72,  80,  81,  91,  92, 113, 114, 118, 121, 122, 123, 124,\n",
       "         129, 130, 131, 136, 151, 152, 155, 161, 167, 168, 177, 178, 179, 189,\n",
       "         213, 214, 220, 221, 222, 223, 224, 227, 228, 229, 248, 249, 254, 255,\n",
       "         284, 285, 286, 289, 291, 292, 299, 300, 301, 302, 337, 338, 339, 340,\n",
       "         344, 345, 368, 373, 374, 375, 376, 377, 382, 384, 390, 391, 394, 395,\n",
       "         400, 401, 402, 410, 413, 414, 415, 417, 418, 419, 422, 427, 442, 443,\n",
       "         444, 445, 449, 450, 456, 458, 471, 472, 473, 474, 506, 507, 508, 509]),\n",
       " tensor([  1,   2,   4,   5,   6,  24,  25,  77,  85,  86,  87,  88,  92,  94,\n",
       "          95, 103, 105, 106, 115, 116, 117, 118, 119, 133, 147, 152, 154, 162,\n",
       "         163, 164, 165, 166, 167, 168, 183, 184, 185, 186, 187, 188, 189, 190,\n",
       "         191, 192, 193, 194, 198, 209, 210, 211, 226, 232, 233, 241, 242, 249,\n",
       "         250, 258, 261, 262, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282,\n",
       "         283, 284, 285, 286, 287, 288, 289, 290, 291, 293, 294, 295, 307, 308,\n",
       "         309, 321, 322])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"ml_position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_wpa_loss(wpa_logits, batch):\n",
    "    print(wpa_logits.shape)\n",
    "    w_logits = wpa_logits[:,:512]\n",
    "    #padとlanguage maskのindexを除外\n",
    "    t  = []\n",
    "    for i in range(wpa_logits.shape[0]):\n",
    "        bool_index = torch.ones(512)\n",
    "        bool_index[batch[\"ml_position\"][i]] = 0\n",
    "        bool_index = bool_index * batch[\"attention_mask\"][i]\n",
    "        t.append(bool_index)\n",
    "    bool_indexes = torch.stack(t).to(torch.bool)\n",
    "    predict_label = w_logits[bool_indexes]\n",
    "    label = batch[\"alignment_label\"][bool_indexes].to(torch.long)\n",
    "    loss = criterion(predict_label, label)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 709, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7797, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_wpa_loss(wpa_logits, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_index = torch.ones(512)\n",
    "bool_index[batch[\"ml_position\"][3]] =0\n",
    "bool_index = bool_index * batch[\"attention_mask\"][3]\n",
    "bool_index.to(torch.bool).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_logits = wpa_logits[:,:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = []\n",
    "for i in range(wpa_logits.shape[0]):\n",
    "    bool_index = torch.ones(512)\n",
    "    bool_index[batch[\"ml_position\"][i]] = 0\n",
    "    bool_index = bool_index * batch[\"attention_mask\"][i]\n",
    "    t.append(bool_index)\n",
    "bool_indexes = torch.stack(t).to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1100)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_indexes.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4168, -0.4560],\n",
       "         [ 0.0144,  0.1555],\n",
       "         [-0.8253, -0.9966],\n",
       "         ...,\n",
       "         [-0.2700, -0.7941],\n",
       "         [ 0.4276, -0.0809],\n",
       "         [ 0.4497, -0.7211]], grad_fn=<IndexBackward0>),\n",
       " torch.Size([4, 512]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_logits[bool_indexes], batch[\"alignment_label\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"alignment_label\"][bool_indexes].to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ml_loss(text_logits, batch):\n",
    "    t = []\n",
    "    for i in range(len(batch[\"ml_position\"])):\n",
    "        if len(batch[\"ml_position\"][i]) == 0:\n",
    "            continue\n",
    "        t.append(text_logits[i][batch[\"ml_position\"][i]])\n",
    "    if len(t) == 0:\n",
    "        return \n",
    "    predict_word_token = torch.cat(t)\n",
    "    labels = torch.cat(batch[\"ml_label\"])\n",
    "    print(predict_word_token.shape)\n",
    "    print(labels.shape)\n",
    "    # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "    loss = criterion(predict_word_token, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([333, 50265])\n",
      "torch.Size([333])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(10.9295, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_ml_loss(text_logits, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([296, 8192])\n",
      "labels torch.Size([296])\n",
      "torch.float32\n",
      "label torch.int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.1631, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_mi_loss(image_logits, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 8192])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_logits[:, 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512, 50265]),\n",
       " torch.Size([4, 197, 8192]),\n",
       " torch.Size([4, 709, 2]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512, 4]), torch.Size([4, 196]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"bbox\"].shape, batch[\"bool_mi_pos\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_aligned_label(bbox, bool_mi_pos):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:, :512].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_logits = logits[:, :512]\n",
    "image_logits = logits[:,513:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63, 50265])\n",
      "torch.Size([63])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11.2611, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_ml_loss(text_logits, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predict_visual_token = image_logits[batch[\"bool_mi_pos\"]].to(torch.float32)\n",
    "    labels = torch.cat(batch[\"mi_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 50265])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_visual_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([300]), torch.Size([300]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_visual_token.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vusual_token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.9910, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(predict_visual_token, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "labels torch.int64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y341sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m mi_loss \u001b[39m=\u001b[39m cal_mi_loss(image_logits, batch)\n",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 16\u001b[0m in \u001b[0;36mcal_mi_loss\u001b[0;34m(image_logits, batch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y341sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(predict_visual_token\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y341sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m, labels\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y341sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predict_visual_token, labels)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y341sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/modules/loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "mi_loss = cal_mi_loss(image_logits, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 50265])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'bbox', 'pixel_values', 'ml_position', 'ml_label', 'bool_mi_pos', 'mi_label', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 50265])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196])\n",
      "torch.Size([196])\n",
      "torch.Size([196])\n",
      "torch.Size([196])\n"
     ]
    }
   ],
   "source": [
    "for b in batch[\"bool_mi_pos\"]:\n",
    "  print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 50265])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_visual_token = image_logits[batch[\"bool_mi_pos\"]]\n",
    "predict_visual_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_visual_token == torch.cat(batch[\"mi_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y232sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39mbool_mi_pos\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape, batch[\u001b[39m\"\u001b[39;49m\u001b[39mmi_label\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "batch[\"bool_mi_pos\"].shape, batch[\"mi_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(batch[\"mi_label\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"mi_label\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"mi_label\"][0] == batch[\"mi_label\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        False, False, False, False, False, False, False, False,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "        False, False,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False, False, False, False, False,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False, False, False, False, False,  True,  True,  True,  True,\n",
       "        False, False, False, False, False, False])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"bool_mi_pos\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        False, False, False, False, False, False, False, False,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False, False,\n",
       "        False, False,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False, False, False, False, False,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False, False, False, False, False,  True,  True,  True,  True,\n",
       "        False, False, False, False, False, False])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"bool_mi_pos\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/pretrain_MIM.ipynb#Y222sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mcat(batch[\u001b[39m\"\u001b[39;49m\u001b[39mbool_mi_pos\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "\u001b[0;31mTypeError\u001b[0m: cat(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "torch.cat(batch[\"bool_mi_pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"bool_mi_pos\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[\"bool_mi_pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True, False, False, False, False, False,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True, False, False, False, False, False,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"bool_mi_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([2, 196])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 196])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] = torch.zeros([196])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[batch[\"bool_mi_pos\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMv3Config {\n",
       "  \"_name_or_path\": \"microsoft/layoutlmv3-base\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"coordinate_size\": 128,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"has_relative_attention_bias\": true,\n",
       "  \"has_spatial_attention_bias\": true,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"input_size\": 224,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_2d_position_embeddings\": 1024,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"max_rel_2d_pos\": 256,\n",
       "  \"max_rel_pos\": 128,\n",
       "  \"model_type\": \"layoutlmv3\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"patch_size\": 16,\n",
       "  \"rel_2d_pos_bins\": 64,\n",
       "  \"rel_pos_bins\": 32,\n",
       "  \"second_input_size\": 112,\n",
       "  \"shape_size\": 128,\n",
       "  \"text_embed\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.21.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"visual_embed\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_visual_bbox( img_size=(14, 14), max_len=1000):\n",
    "    #torch div : divide\n",
    "    visual_bbox_x = torch.div(torch.arange(0, max_len * (img_size[1] + 1), max_len),\n",
    "                              img_size[1], rounding_mode='trunc')\n",
    "    visual_bbox_y = torch.div(torch.arange(0, max_len * (img_size[0] + 1), max_len),\n",
    "                              img_size[0], rounding_mode='trunc')\n",
    "    visual_bbox = torch.stack(\n",
    "        [\n",
    "            visual_bbox_x[:-1].repeat(img_size[0], 1),\n",
    "            visual_bbox_y[:-1].repeat(img_size[1], 1).transpose(0, 1),\n",
    "            visual_bbox_x[1:].repeat(img_size[0], 1),\n",
    "            visual_bbox_y[1:].repeat(img_size[1], 1).transpose(0, 1),\n",
    "        ],\n",
    "        dim=-1,\n",
    "    ).view(-1, 4)\n",
    "    return visual_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,   71,   71],\n",
       "        [  71,    0,  142,   71],\n",
       "        [ 142,    0,  214,   71],\n",
       "        [ 214,    0,  285,   71],\n",
       "        [ 285,    0,  357,   71],\n",
       "        [ 357,    0,  428,   71],\n",
       "        [ 428,    0,  500,   71],\n",
       "        [ 500,    0,  571,   71],\n",
       "        [ 571,    0,  642,   71],\n",
       "        [ 642,    0,  714,   71],\n",
       "        [ 714,    0,  785,   71],\n",
       "        [ 785,    0,  857,   71],\n",
       "        [ 857,    0,  928,   71],\n",
       "        [ 928,    0, 1000,   71],\n",
       "        [   0,   71,   71,  142],\n",
       "        [  71,   71,  142,  142],\n",
       "        [ 142,   71,  214,  142],\n",
       "        [ 214,   71,  285,  142],\n",
       "        [ 285,   71,  357,  142],\n",
       "        [ 357,   71,  428,  142],\n",
       "        [ 428,   71,  500,  142],\n",
       "        [ 500,   71,  571,  142],\n",
       "        [ 571,   71,  642,  142],\n",
       "        [ 642,   71,  714,  142],\n",
       "        [ 714,   71,  785,  142],\n",
       "        [ 785,   71,  857,  142],\n",
       "        [ 857,   71,  928,  142],\n",
       "        [ 928,   71, 1000,  142],\n",
       "        [   0,  142,   71,  214],\n",
       "        [  71,  142,  142,  214],\n",
       "        [ 142,  142,  214,  214],\n",
       "        [ 214,  142,  285,  214],\n",
       "        [ 285,  142,  357,  214],\n",
       "        [ 357,  142,  428,  214],\n",
       "        [ 428,  142,  500,  214],\n",
       "        [ 500,  142,  571,  214],\n",
       "        [ 571,  142,  642,  214],\n",
       "        [ 642,  142,  714,  214],\n",
       "        [ 714,  142,  785,  214],\n",
       "        [ 785,  142,  857,  214],\n",
       "        [ 857,  142,  928,  214],\n",
       "        [ 928,  142, 1000,  214],\n",
       "        [   0,  214,   71,  285],\n",
       "        [  71,  214,  142,  285],\n",
       "        [ 142,  214,  214,  285],\n",
       "        [ 214,  214,  285,  285],\n",
       "        [ 285,  214,  357,  285],\n",
       "        [ 357,  214,  428,  285],\n",
       "        [ 428,  214,  500,  285],\n",
       "        [ 500,  214,  571,  285],\n",
       "        [ 571,  214,  642,  285],\n",
       "        [ 642,  214,  714,  285],\n",
       "        [ 714,  214,  785,  285],\n",
       "        [ 785,  214,  857,  285],\n",
       "        [ 857,  214,  928,  285],\n",
       "        [ 928,  214, 1000,  285],\n",
       "        [   0,  285,   71,  357],\n",
       "        [  71,  285,  142,  357],\n",
       "        [ 142,  285,  214,  357],\n",
       "        [ 214,  285,  285,  357],\n",
       "        [ 285,  285,  357,  357],\n",
       "        [ 357,  285,  428,  357],\n",
       "        [ 428,  285,  500,  357],\n",
       "        [ 500,  285,  571,  357],\n",
       "        [ 571,  285,  642,  357],\n",
       "        [ 642,  285,  714,  357],\n",
       "        [ 714,  285,  785,  357],\n",
       "        [ 785,  285,  857,  357],\n",
       "        [ 857,  285,  928,  357],\n",
       "        [ 928,  285, 1000,  357],\n",
       "        [   0,  357,   71,  428],\n",
       "        [  71,  357,  142,  428],\n",
       "        [ 142,  357,  214,  428],\n",
       "        [ 214,  357,  285,  428],\n",
       "        [ 285,  357,  357,  428],\n",
       "        [ 357,  357,  428,  428],\n",
       "        [ 428,  357,  500,  428],\n",
       "        [ 500,  357,  571,  428],\n",
       "        [ 571,  357,  642,  428],\n",
       "        [ 642,  357,  714,  428],\n",
       "        [ 714,  357,  785,  428],\n",
       "        [ 785,  357,  857,  428],\n",
       "        [ 857,  357,  928,  428],\n",
       "        [ 928,  357, 1000,  428],\n",
       "        [   0,  428,   71,  500],\n",
       "        [  71,  428,  142,  500],\n",
       "        [ 142,  428,  214,  500],\n",
       "        [ 214,  428,  285,  500],\n",
       "        [ 285,  428,  357,  500],\n",
       "        [ 357,  428,  428,  500],\n",
       "        [ 428,  428,  500,  500],\n",
       "        [ 500,  428,  571,  500],\n",
       "        [ 571,  428,  642,  500],\n",
       "        [ 642,  428,  714,  500],\n",
       "        [ 714,  428,  785,  500],\n",
       "        [ 785,  428,  857,  500],\n",
       "        [ 857,  428,  928,  500],\n",
       "        [ 928,  428, 1000,  500],\n",
       "        [   0,  500,   71,  571],\n",
       "        [  71,  500,  142,  571],\n",
       "        [ 142,  500,  214,  571],\n",
       "        [ 214,  500,  285,  571],\n",
       "        [ 285,  500,  357,  571],\n",
       "        [ 357,  500,  428,  571],\n",
       "        [ 428,  500,  500,  571],\n",
       "        [ 500,  500,  571,  571],\n",
       "        [ 571,  500,  642,  571],\n",
       "        [ 642,  500,  714,  571],\n",
       "        [ 714,  500,  785,  571],\n",
       "        [ 785,  500,  857,  571],\n",
       "        [ 857,  500,  928,  571],\n",
       "        [ 928,  500, 1000,  571],\n",
       "        [   0,  571,   71,  642],\n",
       "        [  71,  571,  142,  642],\n",
       "        [ 142,  571,  214,  642],\n",
       "        [ 214,  571,  285,  642],\n",
       "        [ 285,  571,  357,  642],\n",
       "        [ 357,  571,  428,  642],\n",
       "        [ 428,  571,  500,  642],\n",
       "        [ 500,  571,  571,  642],\n",
       "        [ 571,  571,  642,  642],\n",
       "        [ 642,  571,  714,  642],\n",
       "        [ 714,  571,  785,  642],\n",
       "        [ 785,  571,  857,  642],\n",
       "        [ 857,  571,  928,  642],\n",
       "        [ 928,  571, 1000,  642],\n",
       "        [   0,  642,   71,  714],\n",
       "        [  71,  642,  142,  714],\n",
       "        [ 142,  642,  214,  714],\n",
       "        [ 214,  642,  285,  714],\n",
       "        [ 285,  642,  357,  714],\n",
       "        [ 357,  642,  428,  714],\n",
       "        [ 428,  642,  500,  714],\n",
       "        [ 500,  642,  571,  714],\n",
       "        [ 571,  642,  642,  714],\n",
       "        [ 642,  642,  714,  714],\n",
       "        [ 714,  642,  785,  714],\n",
       "        [ 785,  642,  857,  714],\n",
       "        [ 857,  642,  928,  714],\n",
       "        [ 928,  642, 1000,  714],\n",
       "        [   0,  714,   71,  785],\n",
       "        [  71,  714,  142,  785],\n",
       "        [ 142,  714,  214,  785],\n",
       "        [ 214,  714,  285,  785],\n",
       "        [ 285,  714,  357,  785],\n",
       "        [ 357,  714,  428,  785],\n",
       "        [ 428,  714,  500,  785],\n",
       "        [ 500,  714,  571,  785],\n",
       "        [ 571,  714,  642,  785],\n",
       "        [ 642,  714,  714,  785],\n",
       "        [ 714,  714,  785,  785],\n",
       "        [ 785,  714,  857,  785],\n",
       "        [ 857,  714,  928,  785],\n",
       "        [ 928,  714, 1000,  785],\n",
       "        [   0,  785,   71,  857],\n",
       "        [  71,  785,  142,  857],\n",
       "        [ 142,  785,  214,  857],\n",
       "        [ 214,  785,  285,  857],\n",
       "        [ 285,  785,  357,  857],\n",
       "        [ 357,  785,  428,  857],\n",
       "        [ 428,  785,  500,  857],\n",
       "        [ 500,  785,  571,  857],\n",
       "        [ 571,  785,  642,  857],\n",
       "        [ 642,  785,  714,  857],\n",
       "        [ 714,  785,  785,  857],\n",
       "        [ 785,  785,  857,  857],\n",
       "        [ 857,  785,  928,  857],\n",
       "        [ 928,  785, 1000,  857],\n",
       "        [   0,  857,   71,  928],\n",
       "        [  71,  857,  142,  928],\n",
       "        [ 142,  857,  214,  928],\n",
       "        [ 214,  857,  285,  928],\n",
       "        [ 285,  857,  357,  928],\n",
       "        [ 357,  857,  428,  928],\n",
       "        [ 428,  857,  500,  928],\n",
       "        [ 500,  857,  571,  928],\n",
       "        [ 571,  857,  642,  928],\n",
       "        [ 642,  857,  714,  928],\n",
       "        [ 714,  857,  785,  928],\n",
       "        [ 785,  857,  857,  928],\n",
       "        [ 857,  857,  928,  928],\n",
       "        [ 928,  857, 1000,  928],\n",
       "        [   0,  928,   71, 1000],\n",
       "        [  71,  928,  142, 1000],\n",
       "        [ 142,  928,  214, 1000],\n",
       "        [ 214,  928,  285, 1000],\n",
       "        [ 285,  928,  357, 1000],\n",
       "        [ 357,  928,  428, 1000],\n",
       "        [ 428,  928,  500, 1000],\n",
       "        [ 500,  928,  571, 1000],\n",
       "        [ 571,  928,  642, 1000],\n",
       "        [ 642,  928,  714, 1000],\n",
       "        [ 714,  928,  785, 1000],\n",
       "        [ 785,  928,  857, 1000],\n",
       "        [ 857,  928,  928, 1000],\n",
       "        [ 928,  928, 1000, 1000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_init_visual_bbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.0000e-03, 2.0000e-03,  ..., 1.4997e+01, 1.4998e+01,\n",
       "        1.4999e+01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.div(torch.arange(0, 1000 * (14 +1)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_visual_bbox(self, device, dtype, bsz):  # , img_size=(14, 14), max_len=1000):\n",
    "    visual_bbox = self.visual_bbox.repeat(bsz, 1, 1)\n",
    "    visual_bbox = visual_bbox.to(device).type(dtype)\n",
    "    return visual_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.config.has_spatial_attention_bias:\n",
    "                    visual_bbox = self._calc_visual_bbox(device, dtype=torch.long, bsz=batch_size)\n",
    "                    if self.image_only:\n",
    "                        final_bbox = visual_bbox\n",
    "                    else:\n",
    "                        final_bbox = torch.cat([bbox, visual_bbox], dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('exp_005')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46bb9fe42bf29cda1078546925c7ce66ab74e8066732926e47e293312739327"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
