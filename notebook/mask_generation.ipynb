{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "sys.path.append('../src')\n",
    "import  pickle\n",
    "import torch\n",
    "from transformers import LayoutLMv3Tokenizer, AutoConfig, AutoModel, RobertaModel, LayoutLMv3Model\n",
    "from model import LayoutLMv3forMLM, My_DataLoader\n",
    "from utils import utils, masking_generator\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--tokenizer_vocab_dir\", type=str, required=True)\n",
    "parser.add_argument(\"--input_file\", type=str, required=True)\n",
    "parser.add_argument(\"--model_params\", type=str)\n",
    "parser.add_argument(\"--ratio_train\", type=float,default=0.9)\n",
    "parser.add_argument(\"--output_model_dir\", type=str, required=True)\n",
    "parser.add_argument(\"--output_file_name\", type=str, required=True)\n",
    "parser.add_argument(\"--model_name\", type=str, required=True)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "parser.add_argument(\"--leaning_rate\", type=int, default=1e-5)\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1)\n",
    "args_list = [\"--tokenizer_vocab_dir\", \"../data/vocab/tokenizer_vocab/\",\"--input_file\",\n",
    "            \"../data/preprocessing_shared/encoded_dataset.pkl\",\n",
    "            \"--output_model_dir\", \"../data/train/model/\", \\\n",
    "            \"--output_file_name\", \"model.param\", \\\n",
    "            \"--model_name\", \"microsoft/layoutlmv3-base\"]\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LayoutLMv3Tokenizer(f\"{args.tokenizer_vocab_dir}vocab.json\", f\"{args.tokenizer_vocab_dir}merges.txt\")\n",
    "ids = range(tokenizer.vocab_size)\n",
    "vocab = tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.model_params is None:\n",
    "    model = torch.load(args.model_params)\n",
    "else:\n",
    "    config = AutoConfig.from_pretrained(args.model_name)\n",
    "    model = LayoutLMv3forMLM.LayoutLMv3ForMLM(config)\n",
    "    # Roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "    # ## embedidng 層の重みをRobertaの重みで初期化\n",
    "    # weight_size = model.state_dict()[\"model.embeddings.word_embeddings.weight\"].shape\n",
    "    # for i in range(weight_size[0]):\n",
    "    #   model.state_dict()[\"model.embeddings.word_embeddings.weight\"][i] = \\\n",
    "    #   Roberta_model.state_dict()[\"embeddings.word_embeddings.weight\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.input_file, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 16), (14, 14))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = (config.patch_size, config.patch_size)\n",
    "image_size = (config.input_size, config.input_size)\n",
    "patch_shape = (image_size[0] // patch_size[0], image_size[1] // patch_size[1])\n",
    "patch_size, patch_shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = torch.nn.Conv2d(config.num_channels, config.hidden_size, kernel_size=patch_size, stride=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = torch.tensor(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 14, 14])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_emb = proj(pixel_values)\n",
    "vision_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vision_emb.flatten(2).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 14, 14])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = torch.nn.Parameter(torch.zeros(1, 1, config.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_tokens = mask_token.expand(args.batch_size, x.shape[1], -1)\n",
    "mask_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 14])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_masked_pos = torch.zeros([1, 14, 14])\n",
    "bool_masked_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = bool_masked_pos.type_as(mask_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 14])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (768) must match the size of tensor b (14) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/train copy.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m mask_tokens \u001b[39m*\u001b[39;49m w\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (14) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "mask_tokens * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_mask_tokne=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMv3Config {\n",
       "  \"_name_or_path\": \"microsoft/layoutlmv3-base\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"coordinate_size\": 128,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"has_relative_attention_bias\": true,\n",
       "  \"has_spatial_attention_bias\": true,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"input_size\": 224,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_2d_position_embeddings\": 1024,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"max_rel_2d_pos\": 256,\n",
       "  \"max_rel_pos\": 128,\n",
       "  \"model_type\": \"layoutlmv3\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"patch_size\": 16,\n",
       "  \"rel_2d_pos_bins\": 64,\n",
       "  \"rel_pos_bins\": 32,\n",
       "  \"second_input_size\": 112,\n",
       "  \"shape_size\": 128,\n",
       "  \"text_embed\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.21.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_mask_tokne\": true,\n",
       "  \"visual_embed\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayoutLMv3(LayoutLMv3Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "  \n",
    "    def forward_image(self, pixel_values, bool_masked_pos=None):\n",
    "        embeddings = self.patch_embed(pixel_values)\n",
    "        batch_size, seq_len, _ = embeddings.size()\n",
    "        print(embeddings.shape)\n",
    "        print(self.mask_token.shape)\n",
    "\n",
    "        if bool_masked_pos is not None:\n",
    "            mask_tokens = self.mask_token.expand(batch_size, seq_len, -1)\n",
    "            # replace the masked visual tokens by mask_tokens\n",
    "            w = bool_masked_pos.unsqueeze(-1).type_as(mask_tokens)\n",
    "            print(w.shape)\n",
    "            embeddings = embeddings * (1 - w) + mask_tokens * w\n",
    "\n",
    "        # add [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "\n",
    "        # add position embeddings\n",
    "        if self.pos_embed is not None:\n",
    "            embeddings = embeddings + self.pos_embed\n",
    "      \n",
    "\n",
    "\n",
    "        embeddings = self.pos_drop(embeddings)\n",
    "        embeddings = self.norm(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        bbox=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        pixel_values=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        bool_masked_pos=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "        Examples:\n",
    "        ```python\n",
    "        >>> from transformers import AutoProcessor, AutoModel\n",
    "        >>> from datasets import load_dataset\n",
    "        >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "        >>> model = AutoModel.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n",
    "        >>> example = dataset[0]\n",
    "        >>> image = example[\"image\"]\n",
    "        >>> words = example[\"tokens\"]\n",
    "        >>> boxes = example[\"bboxes\"]\n",
    "        >>> encoding = processor(image, words, boxes=boxes, return_tensors=\"pt\")\n",
    "        >>> outputs = model(**encoding)\n",
    "        >>> last_hidden_states = outputs.last_hidden_state\n",
    "        ```\"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            batch_size, seq_length = input_shape\n",
    "            device = input_ids.device\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape\n",
    "            device = inputs_embeds.device\n",
    "        elif pixel_values is not None:\n",
    "            batch_size = len(pixel_values)\n",
    "            device = pixel_values.device\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds or pixel_values\")\n",
    "\n",
    "        if input_ids is not None or inputs_embeds is not None:\n",
    "            if attention_mask is None:\n",
    "                attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n",
    "            if token_type_ids is None:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "            if bbox is None:\n",
    "                bbox = torch.zeros(tuple(list(input_shape) + [4]), dtype=torch.long, device=device)\n",
    "\n",
    "            embedding_output = self.embeddings(\n",
    "                input_ids=input_ids,\n",
    "                bbox=bbox,\n",
    "                position_ids=position_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "\n",
    "        final_bbox = final_position_ids = None\n",
    "        patch_height = patch_width = None\n",
    "        if pixel_values is not None:\n",
    "            patch_height, patch_width = int(pixel_values.shape[2] / self.config.patch_size), int(\n",
    "                pixel_values.shape[3] / self.config.patch_size\n",
    "            )\n",
    "            visual_embeddings = self.forward_image(pixel_values, bool_masked_pos)\n",
    "            visual_attention_mask = torch.ones(\n",
    "                (batch_size, visual_embeddings.shape[1]), dtype=torch.long, device=device\n",
    "            )\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = torch.cat([attention_mask, visual_attention_mask], dim=1)\n",
    "            else:\n",
    "                attention_mask = visual_attention_mask\n",
    "\n",
    "            if self.config.has_relative_attention_bias or self.config.has_spatial_attention_bias:\n",
    "                if self.config.has_spatial_attention_bias:\n",
    "                    visual_bbox = self.calculate_visual_bbox(device, dtype=torch.long, batch_size=batch_size)\n",
    "                    if bbox is not None:\n",
    "                        final_bbox = torch.cat([bbox, visual_bbox], dim=1)\n",
    "                    else:\n",
    "                        final_bbox = visual_bbox\n",
    "\n",
    "                visual_position_ids = torch.arange(\n",
    "                    0, visual_embeddings.shape[1], dtype=torch.long, device=device\n",
    "                ).repeat(batch_size, 1)\n",
    "                if input_ids is not None or inputs_embeds is not None:\n",
    "                    position_ids = torch.arange(0, input_shape[1], device=device).unsqueeze(0)\n",
    "                    position_ids = position_ids.expand(input_shape)\n",
    "                    final_position_ids = torch.cat([position_ids, visual_position_ids], dim=1)\n",
    "                else:\n",
    "                    final_position_ids = visual_position_ids\n",
    "\n",
    "            if input_ids is not None or inputs_embeds is not None:\n",
    "                embedding_output = torch.cat([embedding_output, visual_embeddings], dim=1)\n",
    "            else:\n",
    "                embedding_output = visual_embeddings\n",
    "\n",
    "            embedding_output = self.LayerNorm(embedding_output)\n",
    "            embedding_output = self.dropout(embedding_output)\n",
    "        elif self.config.has_relative_attention_bias or self.config.has_spatial_attention_bias:\n",
    "            if self.config.has_spatial_attention_bias:\n",
    "                final_bbox = bbox\n",
    "            if self.config.has_relative_attention_bias:\n",
    "                position_ids = self.embeddings.position_ids[:, : input_shape[1]]\n",
    "                position_ids = position_ids.expand_as(input_ids)\n",
    "                final_position_ids = position_ids\n",
    "\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n",
    "            attention_mask, None, device, dtype=embedding_output.dtype\n",
    "        )\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            bbox=final_bbox,\n",
    "            position_ids=final_position_ids,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            patch_height=patch_height,\n",
    "            patch_width=patch_width,\n",
    "        )\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output,) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=sequence_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(args.model_name)\n",
    "model = LayoutLMv3(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = patch_shape\n",
    "num_masking_patches = 75\n",
    "max_mask_patches_per_block = None\n",
    "min_mask_patches_per_block = 16\n",
    "\n",
    "# generating mask for the corresponding image\n",
    "mask_generator = masking_generator.MaskingGenerator(\n",
    "            window_size, num_masking_patches=num_masking_patches,\n",
    "            max_num_patches=max_mask_patches_per_block,\n",
    "            min_num_patches=min_mask_patches_per_block,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_masked_pos = mask_generator()\n",
    "bool_masked_pos = torch.from_numpy(bool_masked_pos).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 14])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_masked_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_masked_pos = bool_masked_pos.flatten(1).to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_masked_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True,  True,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True,  True,  True, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True,  True,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True, False, False, False, False, False,  True,  True,  True, False,\n",
       "         False, False,  True,  True,  True, False, False, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " bool_masked_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide into train and valid\n",
    "n_train = math.floor(len(data) * args.ratio_train)\n",
    "train_data = data[:n_train]\n",
    "valid_data = data[n_train:]\n",
    "\n",
    "my_dataloader = My_DataLoader.My_Dataloader(vocab)\n",
    "train_dataloader = my_dataloader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "valid_dataloader = my_dataloader(valid_data, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n",
      "dict_keys(['input_ids', 'bbox', 'pixel_values', 'attention_mask', 'bool_masked_pos'])\n",
      "torch.Size([2, 196, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 196, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/train copy.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m inputs[\u001b[39m\"\u001b[39m\u001b[39mbool_masked_pos\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones([\u001b[39m1\u001b[39m, \u001b[39m14\u001b[39m, \u001b[39m14\u001b[39m])\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/train copy.ipynb Cell 45\u001b[0m in \u001b[0;36mLayoutLMv3.forward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, pixel_values, output_attentions, output_hidden_states, bool_masked_pos, return_dict)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m     bbox\u001b[39m=\u001b[39;49mfinal_bbox,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mfinal_position_ids,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=166'>167</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=169'>170</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=170'>171</a>\u001b[0m     patch_height\u001b[39m=\u001b[39;49mpatch_height,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m     patch_width\u001b[39m=\u001b[39;49mpatch_width,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=172'>173</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=174'>175</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train%20copy.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=176'>177</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/transformers/models/layoutlmv3/modeling_layoutlmv3.py:607\u001b[0m, in \u001b[0;36mLayoutLMv3Encoder.forward\u001b[0;34m(self, hidden_states, bbox, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, position_ids, patch_height, patch_width)\u001b[0m\n\u001b[1;32m    597\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    598\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    599\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         rel_2d_pos,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         output_attentions,\n\u001b[1;32m    612\u001b[0m         rel_pos\u001b[39m=\u001b[39;49mrel_pos,\n\u001b[1;32m    613\u001b[0m         rel_2d_pos\u001b[39m=\u001b[39;49mrel_2d_pos,\n\u001b[1;32m    614\u001b[0m     )\n\u001b[1;32m    616\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/transformers/models/layoutlmv3/modeling_layoutlmv3.py:452\u001b[0m, in \u001b[0;36mLayoutLMv3Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, rel_pos, rel_2d_pos)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    444\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    445\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m     rel_2d_pos\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m ):\n\u001b[0;32m--> 452\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    453\u001b[0m         hidden_states,\n\u001b[1;32m    454\u001b[0m         attention_mask,\n\u001b[1;32m    455\u001b[0m         head_mask,\n\u001b[1;32m    456\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    457\u001b[0m         rel_pos\u001b[39m=\u001b[39;49mrel_pos,\n\u001b[1;32m    458\u001b[0m         rel_2d_pos\u001b[39m=\u001b[39;49mrel_2d_pos,\n\u001b[1;32m    459\u001b[0m     )\n\u001b[1;32m    460\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    462\u001b[0m     outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/transformers/models/layoutlmv3/modeling_layoutlmv3.py:420\u001b[0m, in \u001b[0;36mLayoutLMv3Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, rel_pos, rel_2d_pos)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    412\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    413\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     rel_2d_pos\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    419\u001b[0m ):\n\u001b[0;32m--> 420\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    421\u001b[0m         hidden_states,\n\u001b[1;32m    422\u001b[0m         attention_mask,\n\u001b[1;32m    423\u001b[0m         head_mask,\n\u001b[1;32m    424\u001b[0m         output_attentions,\n\u001b[1;32m    425\u001b[0m         rel_pos\u001b[39m=\u001b[39;49mrel_pos,\n\u001b[1;32m    426\u001b[0m         rel_2d_pos\u001b[39m=\u001b[39;49mrel_2d_pos,\n\u001b[1;32m    427\u001b[0m     )\n\u001b[1;32m    428\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    429\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/transformers/models/layoutlmv3/modeling_layoutlmv3.py:346\u001b[0m, in \u001b[0;36mLayoutLMv3SelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, rel_pos, rel_2d_pos)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    339\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     rel_2d_pos\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m ):\n\u001b[0;32m--> 346\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[1;32m    348\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[1;32m    349\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "model.train()\n",
    "for epoch in range(args.max_epochs):\n",
    "    for iter, batch in enumerate(train_dataloader):\n",
    "        # inputs = {k: v.to(f'cuda:{model.device_ids[0]}') for k in [\"input_ids, bbox\", \"pixel_values\", \"attention_mask\"]}\n",
    "        inputs = {k: batch[k] for k in [\"input_ids\", \"bbox\", \"pixel_values\", \"attention_mask\"]}\n",
    "        inputs[\"bool_masked_pos\"] = torch.ones([1, 14, 14]).flatten(1)\n",
    "        print(inputs.keys())\n",
    "        logits = model.forward(**inputs)\n",
    "        # loss = cal_loss(logits, batch)\n",
    "        # if loss is None:\n",
    "        #     continue\n",
    "        # # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # scheduler.step()\n",
    "        # optimizer.zero_grad()\n",
    "        # losses.append(loss.item())\n",
    "        # if iter % math.floor(iter_per_epoch*0.01) == 0:\n",
    "        #     val_loss = validation()\n",
    "        #     print(iter, loss.item())\n",
    "        #     print(iter,\"val\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([89])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"mask_position\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.87540340423584\n",
      "1 10.913202285766602\n",
      "2 10.854154586791992\n",
      "3 11.003539085388184\n",
      "4 10.977672576904297\n",
      "5 10.788056373596191\n",
      "6 11.0016450881958\n",
      "7 10.83245849609375\n",
      "8 11.038727760314941\n",
      "9 10.941429138183594\n",
      "10 10.746644020080566\n",
      "11 10.765816688537598\n",
      "12 10.817831993103027\n",
      "13 10.815743446350098\n",
      "14 10.858794212341309\n",
      "15 10.890091896057129\n",
      "16 10.817254066467285\n",
      "17 10.730331420898438\n",
      "18 10.704963684082031\n",
      "19 10.630881309509277\n",
      "20 10.737689971923828\n",
      "21 10.534788131713867\n",
      "22 10.640432357788086\n",
      "23 10.627967834472656\n",
      "24 10.67199993133545\n",
      "25 10.671628952026367\n",
      "26 10.399262428283691\n",
      "27 10.393364906311035\n",
      "28 10.457799911499023\n",
      "29 10.454362869262695\n",
      "30 10.372509956359863\n",
      "31 10.474432945251465\n",
      "32 10.55550765991211\n",
      "33 10.313406944274902\n",
      "34 10.34122371673584\n",
      "35 10.251405715942383\n",
      "36 10.119297981262207\n",
      "37 10.092342376708984\n",
      "38 9.98108959197998\n",
      "39 10.217385292053223\n",
      "40 10.075906753540039\n",
      "41 10.293874740600586\n",
      "42 10.036898612976074\n",
      "43 10.015714645385742\n",
      "44 10.167057991027832\n",
      "45 10.199872970581055\n",
      "46 10.429621696472168\n",
      "47 10.076924324035645\n",
      "48 9.979494094848633\n",
      "49 10.155522346496582\n",
      "50 10.099607467651367\n",
      "51 10.06415843963623\n",
      "52 9.898554801940918\n",
      "53 9.956104278564453\n",
      "54 10.250782012939453\n",
      "55 9.723796844482422\n",
      "56 10.266495704650879\n",
      "57 10.050552368164062\n",
      "58 10.083905220031738\n",
      "59 10.11374568939209\n",
      "60 9.909463882446289\n",
      "61 9.795021057128906\n",
      "62 9.783062934875488\n",
      "63 9.848550796508789\n",
      "64 9.72995376586914\n",
      "65 9.830952644348145\n",
      "66 9.856142044067383\n",
      "67 9.835846900939941\n",
      "68 9.896835327148438\n",
      "69 9.898639678955078\n",
      "70 10.040522575378418\n",
      "71 10.011542320251465\n",
      "72 9.658284187316895\n",
      "73 9.8388032913208\n",
      "74 9.912858009338379\n",
      "75 9.657266616821289\n",
      "76 9.821083068847656\n",
      "77 9.738972663879395\n",
      "78 9.752497673034668\n",
      "79 9.824981689453125\n",
      "80 9.89782428741455\n",
      "81 9.73129940032959\n",
      "82 9.808351516723633\n",
      "83 9.797382354736328\n",
      "84 9.938644409179688\n",
      "85 9.453529357910156\n",
      "86 9.27962875366211\n",
      "87 9.342103958129883\n",
      "88 9.56612491607666\n",
      "89 9.708834648132324\n",
      "90 9.587102890014648\n",
      "91 9.890144348144531\n",
      "92 9.592318534851074\n",
      "93 9.892813682556152\n",
      "94 9.353837013244629\n",
      "95 9.73502254486084\n",
      "96 9.768705368041992\n",
      "97 9.713828086853027\n",
      "98 9.604141235351562\n",
      "99 9.57321834564209\n",
      "100 9.761341094970703\n",
      "101 9.352381706237793\n",
      "102 9.335901260375977\n",
      "103 9.292985916137695\n",
      "104 9.389904975891113\n",
      "105 9.472678184509277\n",
      "106 9.54893684387207\n",
      "107 9.266889572143555\n",
      "108 9.554206848144531\n",
      "109 9.578398704528809\n",
      "110 9.50507926940918\n",
      "111 9.345625877380371\n",
      "112 9.447919845581055\n",
      "113 9.219636917114258\n",
      "114 9.447736740112305\n",
      "115 9.348417282104492\n",
      "116 9.700998306274414\n",
      "117 9.468657493591309\n",
      "118 10.09681510925293\n",
      "119 9.253761291503906\n",
      "120 9.44737434387207\n",
      "121 9.402934074401855\n",
      "122 9.07904052734375\n",
      "123 9.475744247436523\n",
      "124 9.04999828338623\n",
      "125 9.404806137084961\n",
      "126 9.256823539733887\n",
      "127 9.48425579071045\n",
      "128 9.316797256469727\n",
      "129 9.225475311279297\n",
      "130 9.44271469116211\n",
      "131 9.520228385925293\n",
      "132 9.070595741271973\n",
      "133 9.32214069366455\n",
      "134 9.56581974029541\n",
      "135 9.330286979675293\n",
      "136 9.365900993347168\n",
      "137 9.183723449707031\n",
      "138 9.386184692382812\n",
      "139 9.240328788757324\n",
      "140 9.132784843444824\n",
      "141 9.8214750289917\n",
      "142 9.358959197998047\n",
      "143 9.275202751159668\n",
      "144 9.261845588684082\n",
      "145 9.575844764709473\n",
      "146 9.654975891113281\n",
      "147 9.812026023864746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/cl/work2/hikaru-si/development/exp_005/notebook/train.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# labels = labels.to(f'cuda:{model.device_ids[0]}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpine12/cl/work2/hikaru-si/development/exp_005/notebook/train.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# losses = []\n",
    "# model.train()\n",
    "# for epoch in range(args.max_epochs):\n",
    "#     for iter, batch in enumerate(dataloader):\n",
    "#         # inputs = {k: v.to(f'cuda:{model.device_ids[0]}') for k in [\"input_ids, bbox\", \"pixel_values\", \"attention_mask\"]}\n",
    "#         inputs = {k: batch[k] for k in [\"input_ids\", \"bbox\", \"pixel_values\", \"attention_mask\"]}\n",
    "#         logits = model.forward(inputs)\n",
    "#         t = []\n",
    "#         for i in range(len(batch[\"mask_position\"])):\n",
    "#             if len(batch[\"mask_position\"][i]) == 0:\n",
    "#                 continue\n",
    "#             t.append(logits[i][batch[\"mask_position\"][i]])\n",
    "#         logits = torch.cat(t)\n",
    "\n",
    "#         labels = torch.cat(batch[\"mask_label\"])\n",
    "#         # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "        \n",
    "#         loss = loss_fn(logits, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.append(loss.item())\n",
    "#         if iter % 4 == 0:\n",
    "#             print(iter, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": args.max_epochs,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"loss_list\": losses,\n",
    "        \"model_state_dict\": model.module.to(\"cpu\").state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    f\"{args.output_model_dir}{args.output_file_name}\",\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(args.output_model_dir+args.output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('exp_005')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46bb9fe42bf29cda1078546925c7ce66ab74e8066732926e47e293312739327"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
