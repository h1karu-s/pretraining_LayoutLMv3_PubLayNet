{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/hikaru-si/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "import os \n",
    "\n",
    "import torch\n",
    "from transformers import LayoutLMv3Tokenizer, AutoConfig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append('../src')\n",
    "from model import My_DataLoader\n",
    "from model.LayoutLMv3forMIM import LayoutLMv3ForPretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--tokenizer_vocab_dir\", type=str, required=True)\n",
    "parser.add_argument(\"--input_file\", type=str, required=True)\n",
    "parser.add_argument(\"--model_params\", type=str)\n",
    "parser.add_argument(\"--ratio_train\", type=float,default=0.9)\n",
    "parser.add_argument(\"--output_model_dir\", type=str, required=True)\n",
    "parser.add_argument(\"--output_file_name\", type=str, required=True)\n",
    "parser.add_argument(\"--model_name\", type=str, required=True)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "parser.add_argument(\"--leaning_rate\", type=int, default=1e-5)\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1)\n",
    "args_list = [\"--tokenizer_vocab_dir\", \"../data/vocab/tokenizer_vocab/\",\"--input_file\",\n",
    "            \"../data/preprocessing_shared/wpa_10000/\",\n",
    "            \"--output_model_dir\", \"../data/train/model/\", \\\n",
    "            \"--output_file_name\", \"model.param\", \\\n",
    "            \"--batch_size\", \"4\", \\\n",
    "            \"--model_name\", \"microsoft/layoutlmv3-base\", \\\n",
    "              \"--model_params\", \"../data/train/pretrain_lr_1e-4_datasiez_10/epoch_1/checkpoint.cpt\"]\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LayoutLMv3Tokenizer(f\"{args.tokenizer_vocab_dir}vocab.json\", f\"{args.tokenizer_vocab_dir}merges.txt\")\n",
    "ids = range(tokenizer.vocab_size)\n",
    "vocab = tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.pkl\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "input_names = os.listdir(args.input_file)\n",
    "for file_name in input_names[0:1]:\n",
    "    print(file_name)\n",
    "    with open(f\"{args.input_file}{file_name}\", \"rb\") as f:\n",
    "        d = pickle.load(f)\n",
    "        data += d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "MaskedLMInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n",
    "#token id から span maskをする\n",
    "#bpe baseではなく word base\n",
    "def create_span_mask_for_ids(token_ids, masked_lm_prob, max_predictions_per_seq, vocab_words, param , rng):\n",
    "    cand_indexes = []\n",
    "    for i, id in enumerate(token_ids):\n",
    "        if id == vocab_words.index(\"<s>\") or id == vocab_words.index(\"</s>\") or id == vocab_words.index(\"<pad>\"):\n",
    "            continue\n",
    "\n",
    "        if len(cand_indexes) >= 1 and not vocab_words[id].startswith(\"Ġ\"):\n",
    "            cand_indexes[-1].append(i)\n",
    "        else:\n",
    "            cand_indexes.append([i])\n",
    "    output_tokens = list(token_ids)\n",
    "    #全単語×0.3(masked_lm_prob)がmaskの対象\n",
    "    num_to_predict = min(max_predictions_per_seq, \n",
    "                      max(1, int(round(len(cand_indexes) * masked_lm_prob))))\n",
    "    \n",
    "\n",
    "    span_count = 0\n",
    "    covered_indexes = [] #mask候補のリスト\n",
    "    covered_set = set()  # 被らないか確かめるための集合\n",
    "    #spanのword数が全words数の30%を超えたら終了\n",
    "    while (span_count < num_to_predict):\n",
    "\n",
    "        span_length = np.random.poisson(lam=param)\n",
    "        if span_count + span_length > num_to_predict or span_length == 0:\n",
    "            continue\n",
    "        #cand_indexesから初めの単語を決める\n",
    "        if len(cand_indexes) -(1 + span_length) <= 0:\n",
    "            break\n",
    "            # continue\n",
    "        start_index = rng.randint(0, len(cand_indexes)-(1 + span_length))\n",
    "        #span_lengthからsubword単位のspanの範囲を決める\n",
    "        covered_index = cand_indexes[start_index: start_index +span_length]\n",
    "        covered_index = list(itertools.chain.from_iterable(covered_index))\n",
    "        if covered_set.isdisjoint(set(covered_index)):\n",
    "            covered_set = covered_set | set(covered_index)\n",
    "            span_count += span_length\n",
    "            # print(span_length)\n",
    "            covered_indexes.append(covered_index)\n",
    "            # print(covered_indexes)\n",
    "\n",
    "    masked_lms = []\n",
    "    for span_index in covered_indexes:\n",
    "        if rng.random() < 0.8:\n",
    "            mask_token_id = vocab_words.index(\"<mask>\")\n",
    "            masked_tokens= [mask_token_id for _ in range(len(span_index))]\n",
    "            #maskした場所と元のtokenを記録\n",
    "            for i in span_index:\n",
    "                masked_lms.append(MaskedLMInstance(index=i, label=token_ids[i]))\n",
    "\n",
    "        else:\n",
    "            if rng.random() < 0.5:\n",
    "                masked_tokens = [token_ids[i] for i in span_index]\n",
    "\n",
    "            else:\n",
    "                #replace words\n",
    "                masked_tokens = [rng.randint(0, len(vocab_words) - 1) for _ in range(len(span_index))]\n",
    "                \n",
    "        for i, index in enumerate(span_index):\n",
    "            output_tokens[index] = masked_tokens[i]\n",
    "\n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_positions = []\n",
    "    masked_lm_labels = []    \n",
    "    for p in masked_lms:\n",
    "        masked_lm_positions.append(p.index)\n",
    "        masked_lm_labels.append(p.label)\n",
    "    \n",
    "    #debag\n",
    "    # if len(token_ids) > 300 and len(masked_lm_positions) < 2:\n",
    "    #     print(f\"error!!! token length: {len(token_ids)}, postions : {masked_lm_positions}, num_to_predict:{num_to_predict}, span_coont:{span_count},covered_indexes:{covered_indexes}, cand_index:{len(cand_indexes)} coverd_indexes_lenght:{len(covered_indexes)}\", flush=True)\n",
    "\n",
    "\n",
    "    return (output_tokens, masked_lm_positions, masked_lm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 7730,\n",
       " 1567,\n",
       " 16,\n",
       " 436,\n",
       " 22662,\n",
       " 334,\n",
       " 903,\n",
       " 1725,\n",
       " 971,\n",
       " 402,\n",
       " 784,\n",
       " 65,\n",
       " 7104,\n",
       " 400,\n",
       " 271,\n",
       " 11482,\n",
       " 285,\n",
       " 685,\n",
       " 16,\n",
       " 11609,\n",
       " 295,\n",
       " 11106,\n",
       " 299,\n",
       " 81,\n",
       " 5384,\n",
       " 91,\n",
       " 446,\n",
       " 903,\n",
       " 1725,\n",
       " 351,\n",
       " 7773,\n",
       " 3045,\n",
       " 342,\n",
       " 1118,\n",
       " 45476,\n",
       " 1365,\n",
       " 397,\n",
       " 627,\n",
       " 944,\n",
       " 557,\n",
       " 1365,\n",
       " 408,\n",
       " 6865,\n",
       " 334,\n",
       " 271,\n",
       " 4387,\n",
       " 285,\n",
       " 271,\n",
       " 816,\n",
       " 285,\n",
       " 2336,\n",
       " 7184,\n",
       " 271,\n",
       " 35359,\n",
       " 860,\n",
       " 2220,\n",
       " 380,\n",
       " 14543,\n",
       " 285,\n",
       " 1783,\n",
       " 295,\n",
       " 11824,\n",
       " 1272,\n",
       " 364,\n",
       " 271,\n",
       " 2826,\n",
       " 334,\n",
       " 903,\n",
       " 1725,\n",
       " 1265,\n",
       " 368,\n",
       " 9893,\n",
       " 295,\n",
       " 5367,\n",
       " 8976,\n",
       " 400,\n",
       " 271,\n",
       " 8585,\n",
       " 295,\n",
       " 1381,\n",
       " 6041,\n",
       " 9823,\n",
       " 299,\n",
       " 3284,\n",
       " 2026,\n",
       " 13,\n",
       " 1705,\n",
       " 5771,\n",
       " 923,\n",
       " 2362,\n",
       " 18,\n",
       " 1283,\n",
       " 478,\n",
       " 351,\n",
       " 476,\n",
       " 6074,\n",
       " 11547,\n",
       " 1821,\n",
       " 17948,\n",
       " 903,\n",
       " 17,\n",
       " 1326,\n",
       " 7730,\n",
       " 7934,\n",
       " 271,\n",
       " 35359,\n",
       " 16,\n",
       " 759,\n",
       " 46099,\n",
       " 334,\n",
       " 923,\n",
       " 2362,\n",
       " 364,\n",
       " 262,\n",
       " 7181,\n",
       " 1603,\n",
       " 4373,\n",
       " 659,\n",
       " 1990,\n",
       " 2925,\n",
       " 275,\n",
       " 295,\n",
       " 4268,\n",
       " 271,\n",
       " 8976,\n",
       " 1385,\n",
       " 342,\n",
       " 979,\n",
       " 342,\n",
       " 673,\n",
       " 903,\n",
       " 1725,\n",
       " 2362,\n",
       " 402,\n",
       " 825,\n",
       " 455,\n",
       " 271,\n",
       " 11482,\n",
       " 4704,\n",
       " 303,\n",
       " 551,\n",
       " 2073,\n",
       " 903,\n",
       " 17,\n",
       " 1326,\n",
       " 7730,\n",
       " 342,\n",
       " 979,\n",
       " 342,\n",
       " 903,\n",
       " 1725,\n",
       " 3383,\n",
       " 299,\n",
       " 958,\n",
       " 509,\n",
       " 13,\n",
       " 406,\n",
       " 380,\n",
       " 814,\n",
       " 18,\n",
       " 271,\n",
       " 16291,\n",
       " 8976,\n",
       " 1385,\n",
       " 376,\n",
       " 3757,\n",
       " 7743,\n",
       " 339,\n",
       " 280,\n",
       " 1519,\n",
       " 303,\n",
       " 12684,\n",
       " 1350,\n",
       " 285,\n",
       " 903,\n",
       " 1725,\n",
       " 3383,\n",
       " 31,\n",
       " 2539,\n",
       " 16,\n",
       " 29580,\n",
       " 408,\n",
       " 2621,\n",
       " 4675,\n",
       " 303,\n",
       " 5744,\n",
       " 339,\n",
       " 286,\n",
       " 5384,\n",
       " 91,\n",
       " 1256,\n",
       " 755,\n",
       " 303,\n",
       " 15137,\n",
       " 339,\n",
       " 271,\n",
       " 923,\n",
       " 1350,\n",
       " 285,\n",
       " 903,\n",
       " 1725,\n",
       " 3383,\n",
       " 295,\n",
       " 7576,\n",
       " 5586,\n",
       " 7730,\n",
       " 18,\n",
       " 436,\n",
       " 18,\n",
       " 23,\n",
       " 18,\n",
       " 1511,\n",
       " 2017,\n",
       " 285,\n",
       " 3638,\n",
       " 1778,\n",
       " 334,\n",
       " 271,\n",
       " 1350,\n",
       " 285,\n",
       " 7181,\n",
       " 1603,\n",
       " 7730,\n",
       " 295,\n",
       " 3968,\n",
       " 17,\n",
       " 90,\n",
       " 535,\n",
       " 2029,\n",
       " 7730,\n",
       " 280,\n",
       " 271,\n",
       " 1029,\n",
       " 18,\n",
       " 87,\n",
       " 601,\n",
       " 2533,\n",
       " 295,\n",
       " 4923,\n",
       " 436,\n",
       " 18,\n",
       " 23,\n",
       " 18,\n",
       " 21,\n",
       " 18,\n",
       " 4373,\n",
       " 17,\n",
       " 1451,\n",
       " 8122,\n",
       " 280,\n",
       " 271,\n",
       " 1029,\n",
       " 18,\n",
       " 87,\n",
       " 18,\n",
       " 17948,\n",
       " 7730,\n",
       " 334,\n",
       " 5997,\n",
       " 2336,\n",
       " 1331,\n",
       " 7181,\n",
       " 1603,\n",
       " 7730,\n",
       " 295,\n",
       " 3968,\n",
       " 17,\n",
       " 90,\n",
       " 535,\n",
       " 2029,\n",
       " 7730,\n",
       " 408,\n",
       " 6072,\n",
       " 342,\n",
       " 32564,\n",
       " 295,\n",
       " 5367,\n",
       " 400,\n",
       " 271,\n",
       " 11996,\n",
       " 285,\n",
       " 7730,\n",
       " 915,\n",
       " 295,\n",
       " 1385,\n",
       " 299,\n",
       " 1279,\n",
       " 5802,\n",
       " 13,\n",
       " 280,\n",
       " 271,\n",
       " 273,\n",
       " 598,\n",
       " 285,\n",
       " 271,\n",
       " 12428,\n",
       " 18,\n",
       " 1650,\n",
       " 1767,\n",
       " 8122,\n",
       " 334,\n",
       " 11824,\n",
       " 10881,\n",
       " 659,\n",
       " 476,\n",
       " 657,\n",
       " 7104,\n",
       " 428,\n",
       " 273,\n",
       " 598,\n",
       " 16,\n",
       " 271,\n",
       " 12428,\n",
       " 510,\n",
       " 87,\n",
       " 1819,\n",
       " 351,\n",
       " 7579,\n",
       " 280,\n",
       " 271,\n",
       " 8122,\n",
       " 4270,\n",
       " 28362,\n",
       " 1118,\n",
       " 4029,\n",
       " 3638,\n",
       " 364,\n",
       " 938,\n",
       " 2519,\n",
       " 2776,\n",
       " 285,\n",
       " 7730,\n",
       " 971,\n",
       " 1930,\n",
       " 400,\n",
       " 271,\n",
       " 926,\n",
       " 280,\n",
       " 3485,\n",
       " 402,\n",
       " 813,\n",
       " 455,\n",
       " 7181,\n",
       " 1603,\n",
       " 4373,\n",
       " 295,\n",
       " 3968,\n",
       " 17,\n",
       " 90,\n",
       " 535,\n",
       " 2029,\n",
       " 7730,\n",
       " 408,\n",
       " 1097,\n",
       " 271,\n",
       " 9017,\n",
       " 285,\n",
       " 431,\n",
       " 10897,\n",
       " 18,\n",
       " 3274,\n",
       " 7181,\n",
       " 1603,\n",
       " 7730,\n",
       " 16,\n",
       " 273,\n",
       " 598,\n",
       " 7104,\n",
       " 262,\n",
       " 1778,\n",
       " 8122,\n",
       " 28362,\n",
       " 1118,\n",
       " 30483,\n",
       " 334,\n",
       " 7181,\n",
       " 1603,\n",
       " 7730,\n",
       " 334,\n",
       " 5997,\n",
       " 994,\n",
       " 11688,\n",
       " 971,\n",
       " 402,\n",
       " 746,\n",
       " 65,\n",
       " 280,\n",
       " 2399,\n",
       " 16,\n",
       " 557,\n",
       " 3462,\n",
       " 2354,\n",
       " 9913,\n",
       " 334,\n",
       " 1783,\n",
       " 815,\n",
       " 299,\n",
       " 12683,\n",
       " 387,\n",
       " 13,\n",
       " 295,\n",
       " 11824,\n",
       " 2776,\n",
       " 334,\n",
       " 7181,\n",
       " 1603,\n",
       " 7730,\n",
       " 18,\n",
       " 9913,\n",
       " 334,\n",
       " 923,\n",
       " 2776,\n",
       " 408,\n",
       " 476,\n",
       " 1713,\n",
       " 18,\n",
       " 271,\n",
       " 273,\n",
       " 598,\n",
       " 860,\n",
       " 7104,\n",
       " 262,\n",
       " 8223,\n",
       " 8122,\n",
       " 28362,\n",
       " 1118,\n",
       " 5810,\n",
       " 303,\n",
       " 2768,\n",
       " 364,\n",
       " 7181,\n",
       " 1603,\n",
       " 7730,\n",
       " 334,\n",
       " 10130,\n",
       " 5997,\n",
       " 994,\n",
       " 11688,\n",
       " 971,\n",
       " 402,\n",
       " 729,\n",
       " 65,\n",
       " 280,\n",
       " 6276,\n",
       " 1167,\n",
       " 364,\n",
       " 680,\n",
       " 1385,\n",
       " 17,\n",
       " 15750,\n",
       " 339,\n",
       " 673,\n",
       " 2307,\n",
       " 285,\n",
       " 5280,\n",
       " 7730,\n",
       " 295,\n",
       " 1603,\n",
       " 17,\n",
       " 1326,\n",
       " 3673,\n",
       " 3383,\n",
       " 18,\n",
       " 342,\n",
       " 271,\n",
       " 8122,\n",
       " 860,\n",
       " 1713,\n",
       " 271,\n",
       " 6952,\n",
       " 303,\n",
       " 12890,\n",
       " 2081,\n",
       " 337,\n",
       " 923,\n",
       " 788,\n",
       " 16,\n",
       " 7237,\n",
       " 923,\n",
       " 2362,\n",
       " 334,\n",
       " 7181,\n",
       " 1603,\n",
       " 4373,\n",
       " 860,\n",
       " 657,\n",
       " 2281,\n",
       " 18,\n",
       " 2034,\n",
       " 2402,\n",
       " 295,\n",
       " 2945,\n",
       " 860,\n",
       " 657,\n",
       " 14383,\n",
       " 280,\n",
       " 271,\n",
       " 273,\n",
       " 598,\n",
       " 16,\n",
       " 884,\n",
       " 7104,\n",
       " 262,\n",
       " 6122,\n",
       " 8122,\n",
       " 280,\n",
       " 2399,\n",
       " 402,\n",
       " 1052,\n",
       " 16,\n",
       " 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  args.model_params is not None:\n",
    "    checkpoint = torch.load(args.model_params, map_location=torch.device('cpu'))\n",
    "    config = AutoConfig.from_pretrained(args.model_name)\n",
    "    config.num_visual_tokens = 8192\n",
    "    model = LayoutLMv3ForPretraining(config)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    config = AutoConfig.from_pretrained(args.model_name)\n",
    "    config.num_visual_tokens = 8192\n",
    "    model = LayoutLMv3ForPretraining(config)\n",
    "    # Roberta_model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "    # ## embedidng 層の重みをRobertaの重みで初期化\n",
    "    # weight_size = model.state_dict()[\"model.embeddings.word_embeddings.weight\"].shape\n",
    "    # for i in range(weight_size[0]):\n",
    "    #   model.state_dict()[\"model.embeddings.word_embeddings.weight\"][i] = \\\n",
    "    #   Roberta_model.state_dict()[\"embeddings.word_embeddings.weight\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512]), torch.Size([124]), torch.Size([124]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(12)\n",
    "np.random.seed(12)\n",
    "ids, pos , lab = create_span_mask_for_ids(data[2][\"input_ids\"], 0.3, 153, vocab, 1, random)\n",
    "ids = torch.tensor(ids)\n",
    "pos = torch.tensor(pos)\n",
    "lab = torch.tensor(lab)\n",
    "ids.shape, pos.shape , lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in ids:\n",
    "  if i == 4:\n",
    "    cnt += 1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  1,   9,  13,  14,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,\n",
       "          45,  46,  55,  65,  66,  67,  76,  77,  78,  79,  82,  83,  86,  87,\n",
       "          88,  89,  90,  91,  92, 108, 109, 110, 123, 127, 128, 138, 147, 156,\n",
       "         157, 162, 163, 164, 165, 166, 167, 168, 169, 175, 176, 183, 197, 201,\n",
       "         205, 208, 209, 210, 211, 212, 213, 228, 229, 233, 234, 248, 249, 255,\n",
       "         258, 265, 266, 269, 270, 276, 281, 282, 283, 284, 285, 286, 299, 300,\n",
       "         311, 313, 314, 315, 321, 322, 334, 335, 341, 343, 346, 347, 357, 364,\n",
       "         368, 371, 378, 379, 380, 386, 394, 395, 396, 397, 398, 407, 428, 429,\n",
       "         436, 437, 447, 456, 457, 462, 463, 464, 469, 479, 480, 481]),\n",
       " tensor([    0,     4, 34922, 45492, 18593,   272,    30,    29,  5374,     4,\n",
       "         13589, 11654,  3493,     4,     4,  6153,    17,  2708,  4864,   334,\n",
       "             4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
       "         19441,    81,    21,  6351,  6843, 20040, 11348,   466,    16, 19441,\n",
       "            81,    16,  8313,    22,    16,     4,     4,   431,   383,  8457,\n",
       "          6918,    94,    16,  8313,    23,     4, 10304,   263,   596,  2782,\n",
       "         13382,    16, 19441,    81,    16,     4,     4,     4, 14026,    22,\n",
       "           337,  2861,   285,   994,   815,   295,     4,     4,     4,     4,\n",
       "          3023,    16,     4,     4,  8249,    31,     4,     4,     4,     4,\n",
       "             4,     4,     4,  2483,  8831,  3023,    16, 25188,    16,  8249]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos, ids[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([44427,   428,   848,  6761, 38621,  3565,   287,  7698,  1446, 14340,\n",
       "          1841,   613,   284,    16,  2011, 14590,   295,  8313,    16, 14735,\n",
       "          8040,    16,  2483,  8831, 25188,    16,   335,   522,   626,   378,\n",
       "           285, 39276,    16,  2087,  9823,    16,   262,   659,  4013,   285,\n",
       "         38621,  4153,    18,   272,    30,    29,  1122,   406,   271,   814,\n",
       "           285,   428,  1606,   271,  1351,   295,  1272,   295,   546,    18,\n",
       "         34922, 45492,   280,   293,    18,   295,   271,  1801,  1017,   807,\n",
       "         49925,  4464,    18, 38041,    31, 15315, 10835,   934,    31, 44427,\n",
       "         34922, 45492,  3424,    30,   310,  5778,   285,   271, 12795,   339,\n",
       "          1253, 38041,  1577, 26188,   295,   262,   351, 38621,   351,  2699,\n",
       "         12530,   325,    16,   271,  6725,    16,  9497,  1206,    16,   271,\n",
       "           285,  2336,  5116,    16,  1694, 36989,   295,   271,  5125,   285,\n",
       "           271, 11239,   280,  1690]),\n",
       " tensor([44427,   428,   848,  6761, 38621,  3565,   287,  7698,  1446, 14340,\n",
       "          1841,   613,   284,    16,  2011, 14590,   295,  8313,    16, 14735,\n",
       "          8040,    16,  2483,  8831, 25188,    16,   335,   522,   626,   378,\n",
       "           285, 39276,    16,  2087,  9823,    16,   262,   659,  4013,   285,\n",
       "         38621,  4153,    18,   272,    30,    29,  1122,   406,   271,   814,\n",
       "           285,   428,  1606,   271,  1351,   295,  1272,   295,   546,    18,\n",
       "         34922, 45492,   280,   293,    18,   295,   271,  1801,  1017,   807,\n",
       "         49925,  4464,    18, 38041,    31, 15315, 10835,   934,    31, 44427,\n",
       "         34922, 45492,  3424,    30,   310,  5778,   285,   271, 12795,   339,\n",
       "          1253, 38041,  1577, 26188,   295,   262,   351, 38621,   351,  2699,\n",
       "         12530,   325,    16,   271,  6725,    16,  9497,  1206,    16,   271,\n",
       "           285,  2336,  5116,    16,  1694, 36989,   295,   271,  5125,   285,\n",
       "           271, 11239,   280,  1690]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo = torch.zeros(512)\n",
    "for i in pos:\n",
    "  bo[i] = 1\n",
    "bo  = bo.to(torch.bool)\n",
    "lab  , torch.tensor(data[][\"input_ids\"])[bo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "my_dataloader = My_DataLoader.My_Dataloader(vocab, random)\n",
    "dataloader = my_dataloader(data, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/hikaru-si/.pyenv/versions/3.8.6/envs/exp_005/lib/python3.8/site-packages/transformers/modeling_utils.py:713: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "# model.train()\n",
    "for epoch in range(args.max_epochs):\n",
    "    for iter, batch in enumerate(dataloader):\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        inputs = {k: batch[k] for k in [\"input_ids\", \"bbox\", \"pixel_values\", \"attention_mask\", \"bool_mi_pos\"]}\n",
    "        text_logits, image_logits, wpa_logits = model.forward(inputs)\n",
    "        break\n",
    "        \n",
    "        # for i in range(batch[\"input_ids\"].shape[0]):\n",
    "            \n",
    "        #     for j in batch[\"ml_position\"][i]:\n",
    "        #         bo = torch.zeros(512)\n",
    "        #         if batch[\"input_ids\"][i][j] != 4:\n",
    "        #             print(f\"{i}, {j} \")\n",
    "        #             break\n",
    "        #         bo[j] = 1\n",
    "        #     bo = bo.to(torch.bool)\n",
    "        #     a = (batch[\"input_ids\"][i][bo] == batch[\"ml_label\"][i]).sum()\n",
    "        #     print(a)\n",
    "        #     if a == False:\n",
    "        #         break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 512, 50265])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"][0][batch[\"ml_position\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 3],\n",
       "         [2, 4]],\n",
       "\n",
       "        [[5, 6],\n",
       "         [7, 8]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.stack([torch.tensor([[1,3], [2, 4]]), torch.tensor([[5,6], [7, 8]])])\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5, 6],\n",
       "         [7, 8]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[torch.tensor([1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ml_loss(text_logits, batch):\n",
    "    t = []\n",
    "    for i in range(len(batch[\"ml_position\"])):\n",
    "        if len(batch[\"ml_position\"][i]) == 0:\n",
    "            continue\n",
    "        t.append(text_logits[i][batch[\"ml_position\"][i]])\n",
    "    if len(t) == 0:\n",
    "        return \n",
    "    predict_word_token = torch.cat(t)\n",
    "    labels = torch.cat(batch[\"ml_label\"])\n",
    "    print(predict_word_token.shape)\n",
    "    print(labels.shape)\n",
    "    # labels = labels.to(f'cuda:{model.device_ids[0]}')\n",
    "    loss = criterion(predict_word_token + 1e-12, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.arange(512)\n",
    "\n",
    "torch.cat([torch.arange(10), torch.arange(10,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo = torch.zeros(512)\n",
    "for i in batch[\"ml_position\"][2]:\n",
    "  bo[i] = 1\n",
    "bo  = bo.to(torch.bool)\n",
    "batch[\"ml_label\"][2], batch[\"input_ids\"][2][bo].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('exp_005')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46bb9fe42bf29cda1078546925c7ce66ab74e8066732926e47e293312739327"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
